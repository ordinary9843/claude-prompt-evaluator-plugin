---
name: fix
disable-model-invocation: true
description: Reviews any Claude Code artifact (SKILL.md, agent .md, .claude/commands/*.md, MEMORY.md, output styles, context files, .claude/rules/*.md, CLAUDE.md), then automatically generates an improved version targeting the highest-impact issues. Shows a before/after diff and asks for confirmation before writing. Follows the additive-only principle: never removes existing content. Auto-detects the file type from the path. Use when the user asks to "fix", "improve", "auto-fix", "enhance", "optimise", "rewrite", or "make better" any Claude Code markdown file — or when a review score is below 80 and the user wants changes applied automatically.
---

# Fix Any Claude Code Artifact

Runs a review, then proposes and applies targeted improvements — both structural and semantic.

## Step 1 — Detect target and score it

Accept `$ARGUMENTS` as a file path. If no argument, scan the current directory for a single Claude Code artifact in this priority order: SKILL.md → agents/*.md → .claude/commands/*.md → MEMORY.md → .claude/output-styles/*.md → .claude/rules/*.md → CLAUDE.md. If multiple candidates exist, ask the user to specify.

If `$ARGUMENTS` is provided but the path does not exist, report: `Error: '<path>' not found — stopping.`
If `$ARGUMENTS` is provided and the path does not end in `.md`, report: `Error: '<path>' is not a .md file — only markdown artifacts are supported.`
If the detected file type is not a recognized artifact type, report: `Error: '<path>' is not a recognized artifact type — stopping.`

Use the same auto-detection logic as the `review` skill to identify the file type.

Load the appropriate `skills/review/references/<type>-criteria.md` file for the detected artifact type (same routing as the `review` skill's Step 2 table: `skill`, `agent`, `claude-md`, `rule`, `command`, `memory`, `output-style`, `context`) and apply its rubric internally. Do not print the review — store the scores and issue list.

If the file has a blocking error (invalid YAML frontmatter), report it and stop. The file must be parseable before it can be improved.

If no criteria file exists at `skills/review/references/<type>-criteria.md` for the detected type, report: `Error: no criteria file found for type '<type>' — stopping.`

## Step 2 — Select improvement targets

Identify issues ranked by point impact (highest first). Select the top issues that together account for ≥ 20 points of improvement, or all issues if total gap is < 20 points.

For each selected issue, note:
- Exact location (field, line, or section)
- Current value
- Proposed value
- Points gained

Skip issues that require user-specific knowledge (e.g. homepage URL, author name). Use `<TODO: ...>` placeholders for those.

## Step 2b — Challenge the proposed fixes

Before showing the plan, internally evaluate each proposed fix from a devil's advocate perspective (inspired by the `challenger` agent's principles, but without delegating to it):

For each proposed fix, check these dimensions:
- **Preserve intent**: Verify the fix does not strip away intentional nuance or domain-specific choices.
- **Assess token cost**: Determine if the fix increases context window usage disproportionately to the quality gain.
- **Evaluate original design**: Determine if a valid architectural reason exists for the current approach; if so, annotate with [!].

Annotate the fix plan with challenger findings:
- [!] marks fixes where the original approach might be intentionally better — user decides
- Fixes without warnings are safe to apply

## Step 3 — Show the plan

Before generating the improved file, output:

```markdown
## Fix Plan: `<path>`

Current score: XX/100 (Grade X)
Projected score after fixes: ~XX/100 (Grade X)

Changes to apply:
N. [+X pts] <field or section>
   Before: <current value>
   After:  <proposed value>
   [!] <challenger note, if any>
```

Ask: **"Apply these changes? (yes / no / show full file first)"**

- `yes` → proceed to Step 4
- `no` → stop, no files modified
- `show full file first` → output the complete improved file in Step 3b, then ask again
- Any other response → ask again: "Please reply with yes, no, or show full file first."

## Step 3b — Show full improved file (optional)

If the user asks to see it first, output the complete improved file content in a fenced code block with the correct language tag:
- `.md` files → ` ```markdown `

## Step 4 — Write the improved file

Write the improved content to the original file path.

Then output the final score:

```text
Fixed. New score: ~XX/100 (Grade X)

Remaining issues (not auto-fixable):
- <issue requiring user-specific knowledge or architectural decision>
```

## Constraints

- Always confirm before writing.
- Never delete existing content.
- Use `<TODO: ...>` for anything requiring user knowledge.
- If the file scores ≥ 90 and the user did not explicitly request a fix, report the score, congratulate the user, and stop.
- If the file scores 80–89 (gap < 20), fix all identified issues.
- If the user explicitly requests a fix despite a score ≥ 90, acknowledge the high score but proceed with any remaining minor improvements.

## Improvement Rules

Load `skills/fix/references/fix-criteria.md` for per-file-type improvement rules and apply them.

---

## Example Flow

**Input**: `agents/challenger.md`

**Step 1** — Type detected: `agent`. Internal review (via `references/agent-criteria.md`): 89/100 (Grade B).

**Step 2** — Issues ranked by impact (gap = 11 pts):
- [+8 pts] Behavioral Constraints — no anti-hallucination guard present
- [+3 pts] No Input / Output section — what the agent receives and returns is unspecified

**Step 3 — Fix Plan shown to user:**

```text
Current score: 89/100 (Grade B)
Projected score after fixes: ~97/100 (Grade A)

Changes to apply:
1. [+8 pts] Behavioral Constraints — add anti-hallucination guard
   Before: (absent)
   After:  "Ground every challenge in the artifact. Only challenge based on
            content actually present in the provided artifact…"
2. [+3 pts] Add "## Input / Output" section
   Before: (absent)
   After:  "You receive: a Claude Code artifact or a review score table.
            You must produce: the structured debate format defined below."
```

Apply these changes? (yes / no / show full file first)

**Step 4** — After user confirms `yes`: file written.

```text
Fixed. New score: ~97/100 (Grade A)

Remaining issues (not auto-fixable):
- None for this file.
```

---

**Example: file already scores ≥ 90**

**Input**: `CLAUDE.md`

**Step 1** — Type detected: `claude-md`. Internal review: 98/100 (Grade A).

Score is ≥ 90 — no changes needed.

```text
CLAUDE.md scores 98/100 (Grade A) — excellent quality. No fixes applied.
```

---

**Example: user declines**

After the fix plan is shown for `agents/challenger.md`:

> Apply these changes? `no`

```text
No changes made. agents/challenger.md was not modified.
```
